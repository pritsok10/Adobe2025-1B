import os
import json
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer, util
import torch
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import shutil

# --- STAGE 1: OUTLINE EXTRACTION ---
# We will import and use the process_pdfs function from your goofrun1.py file.
# This assumes goofrun1.py is in the same directory.
try:
    from goofrun1 import process_pdfs as run_outline_extraction
    print("Successfully imported 'process_pdfs' from goofrun1.py for Stage 1.")
except ImportError:
    print("Error: Could not import 'process_pdfs' from goofrun1.py.")
    print("Please ensure 'goofrun1.py' is in the same directory as this script.")
    # Define a dummy function to prevent crashing if the import fails
    def run_outline_extraction(input_dir: str, output_dir: str):
        print("Stage 1 was skipped due to import error.")
        pass

# --- STAGE 2: INTELLIGENT ANALYSIS ---
# This section uses the outlines generated by Stage 1 to find relevant content.

class IntelligentDocumentAnalyst:
    """
    Analyzes pre-extracted document outlines to find sections relevant
    to a persona and job, then produces the final output.
    """
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model.to(self.device)
        print(f"Intelligent Analyst using device: {self.device}")

    def _extract_text_under_heading(self, pdf_path: str, full_outline: List[Dict], heading_index: int) -> str:
        """Extracts text content between a heading and the next one."""
        doc = fitz.open(pdf_path)
        start_heading = full_outline[heading_index]
        start_page = start_heading['page'] - 1
        
        end_page = len(doc) - 1
        end_y = doc[end_page].rect.height
        if heading_index + 1 < len(full_outline):
            next_heading = full_outline[heading_index + 1]
            end_page = next_heading['page'] - 1
            if end_page >= 0:
                page_for_next = doc.load_page(end_page)
                # Search for the next heading's text to find its y-coordinate
                text_instances = page_for_next.search_for(next_heading['text'])
                if text_instances:
                    end_y = text_instances[0].y0

        extracted_text = []
        for i in range(start_page, end_page + 1):
            page = doc.load_page(i)
            # Define the clip area for text extraction. Bbox is expected from goofrun1.py
            start_y = start_heading['bbox'][3] if i == start_page and 'bbox' in start_heading and start_heading['bbox'] else 0
            stop_y = end_y if i == end_page else page.rect.height
            
            clip_rect = fitz.Rect(0, start_y, page.rect.width, stop_y)
            text = page.get_text(clip=clip_rect).strip()
            if text:
                extracted_text.append(text)
        
        doc.close()
        return "\n\n".join(extracted_text)

    def analyze_outlines(self, pdf_dir: str, outline_dir: str, persona: str, job_to_be_done: str) -> Dict:
        """
        Takes a directory of JSON outlines, finds relevant sections, and returns the final JSON output.
        """
        print("\n--- Running Stage 2: Intelligent Analysis ---")
        outline_files = list(Path(outline_dir).glob("*.json"))
        if not outline_files:
            print("No outline files found to analyze. Stage 1 might have failed or found no PDFs.")
            return {}

        all_headings = []
        outlines_by_doc = {}
        pdf_files = [f"{f.stem}.pdf" for f in outline_files]

        for outline_file in outline_files:
            doc_name = f"{outline_file.stem}.pdf"
            try:
                with open(outline_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # goofrun1.py might produce JSONs without an 'outline' key if it fails.
                    outline = data.get("outline", [])
                    if not outline:
                        print(f"Warning: No outline found in {outline_file.name}")
                        continue
                    outlines_by_doc[doc_name] = outline
                    for i, heading in enumerate(outline):
                        heading['document'] = doc_name
                        heading['original_index'] = i
                        all_headings.append(heading)
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Warning: Could not read or parse {outline_file.name}. Error: {e}")


        if not all_headings:
            print("No valid headings found in any outline files. Cannot proceed.")
            return {}

        query = f"{persona} {job_to_be_done}"
        query_embedding = self.model.encode(query, convert_to_tensor=True, device=self.device)
        
        heading_texts = [h['text'] for h in all_headings]
        heading_embeddings = self.model.encode(heading_texts, convert_to_tensor=True, device=self.device, show_progress_bar=True)
        
        cos_scores = util.pytorch_cos_sim(query_embedding, heading_embeddings)[0]
        top_results = torch.topk(cos_scores, k=min(5, len(all_headings)))
        
        extracted_sections = []
        subsection_analysis = []
        
        print("\nExtracting text for top 5 relevant sections...")
        for i, (score, idx) in enumerate(zip(top_results.values, top_results.indices)):
            heading_info = all_headings[idx]
            print(f"  Rank {i+1}: '{heading_info['text']}' in '{heading_info['document']}' (Score: {score:.4f})")
            
            pdf_path = os.path.join(pdf_dir, heading_info['document'])
            full_outline = outlines_by_doc[heading_info['document']]
            refined_text = self._extract_text_under_heading(pdf_path, full_outline, heading_info['original_index'])

            extracted_sections.append({
                "document": heading_info['document'], "section_title": heading_info['text'],
                "importance_rank": i + 1, "page_number": heading_info['page']
            })
            subsection_analysis.append({
                "document": heading_info['document'], "refined_text": refined_text,
                "page_number": heading_info['page']
            })
        
        print("--- Stage 2 Complete ---")
        return {
            "metadata": {"input_documents": pdf_files, "persona": persona, "job_to_be_done": job_to_be_done, "processing_timestamp": datetime.now().isoformat()},
            "extracted_sections": extracted_sections,
            "subsection_analysis": subsection_analysis
        }


if __name__ == '__main__':
    # --- Configuration ---
    PDF_INPUT_DIR = "Collection/pdf"
    INTERMEDIATE_OUTLINE_DIR = "Collection/Outputgoof2" 
    FINAL_OUTPUT_FILE = 'challenge1b_output.json'

    PERSONA = "Travel Planner"
    JOB_TO_BE_DONE = "Plan a trip of 4 days for a group of 10 college friends."

    # --- Pipeline Execution ---
    
    # Clean up intermediate directory if it exists to ensure a fresh run
    if os.path.exists(INTERMEDIATE_OUTLINE_DIR):
        shutil.rmtree(INTERMEDIATE_OUTLINE_DIR)

    # STAGE 1: Run the outline extraction on the PDF directory using the imported function
    print("--- Running Stage 1: Outline Extraction (using imported process_pdfs) ---")
    run_outline_extraction(input_dir=PDF_INPUT_DIR, output_dir=INTERMEDIATE_OUTLINE_DIR)
    print("--- Stage 1 Complete ---")

    # STAGE 2: Run the intelligent analysis on the generated outlines
    analyst = IntelligentDocumentAnalyst()
    final_result = analyst.analyze_outlines(
        pdf_dir=PDF_INPUT_DIR,
        outline_dir=INTERMEDIATE_OUTLINE_DIR,
        persona=PERSONA,
        job_to_be_done=JOB_TO_BE_DONE
    )

    # Save the final result
    if final_result:
        with open(FINAL_OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=4, ensure_ascii=False)
        print(f"\nâœ… Pipeline complete. Final output saved to {FINAL_OUTPUT_FILE}")

    # Clean up the intermediate directory
    if os.path.exists(INTERMEDIATE_OUTLINE_DIR):
        shutil.rmtree(INTERMEDIATE_OUTLINE_DIR)